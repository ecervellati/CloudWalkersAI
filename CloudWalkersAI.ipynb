{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iGUrmB-pWb-p",
        "yBJ1pK8sD-dE",
        "OGPOzAcLGALf"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1UoRvY30mV6"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/crowsonkb/k-diffusion.git\n",
        "%cd k-diffusion\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! ./train.py --config /content/k-diffusion/configs/config_cifar10.json --name RUN_NAME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi-Nz4Nf0ufd",
        "outputId": "1c312f09-beaa-4e65-fc19-8a416c128c2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0 using device: cpu\n",
            "Parameters: 66121860\n",
            "Files already downloaded and verified\n",
            "Number of items in dataset: 50000\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Computing features for reals...\n",
            "100% 32/32 [11:09<00:00, 20.93s/it]\n",
            "Epoch: 0, step: 0, loss: 0.500053\n",
            "Sampling...\n",
            "  0% 0/781 [01:12<?, ?it/s]\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 1/50 [00:19<16:05, 19.70s/it]\u001b[A\n",
            "  4% 2/50 [00:39<15:42, 19.64s/it]\u001b[A\n",
            "  6% 3/50 [01:00<16:07, 20.58s/it]\u001b[A\n",
            "  8% 4/50 [01:20<15:28, 20.19s/it]\u001b[A\n",
            " 10% 5/50 [01:40<14:59, 19.98s/it]\u001b[A\n",
            " 12% 6/50 [01:59<14:33, 19.85s/it]\u001b[A\n",
            " 14% 7/50 [02:19<14:13, 19.85s/it]\u001b[A\n",
            " 16% 8/50 [02:39<13:50, 19.77s/it]\u001b[A\n",
            " 18% 9/50 [03:00<13:54, 20.35s/it]\u001b[A\n",
            " 20% 10/50 [03:20<13:25, 20.13s/it]\u001b[A\n",
            " 22% 11/50 [03:40<12:59, 19.97s/it]\u001b[A\n",
            " 24% 12/50 [03:59<12:34, 19.87s/it]\u001b[A\n",
            " 26% 13/50 [04:19<12:11, 19.77s/it]\u001b[A\n",
            " 28% 14/50 [04:38<11:49, 19.70s/it]\u001b[A\n",
            " 30% 15/50 [05:00<11:52, 20.35s/it]\u001b[A\n",
            " 32% 16/50 [05:20<11:24, 20.13s/it]\u001b[A\n",
            " 34% 17/50 [05:39<10:59, 19.98s/it]\u001b[A\n",
            " 36% 18/50 [05:59<10:35, 19.86s/it]\u001b[A\n",
            " 38% 19/50 [06:19<10:12, 19.77s/it]\u001b[A\n",
            " 40% 20/50 [06:38<09:51, 19.71s/it]\u001b[A\n",
            " 42% 21/50 [07:02<10:10, 21.04s/it]\u001b[A\n",
            " 44% 22/50 [07:22<09:38, 20.65s/it]\u001b[A\n",
            " 46% 23/50 [07:42<09:08, 20.32s/it]\u001b[A\n",
            " 48% 24/50 [08:01<08:42, 20.10s/it]\u001b[A\n",
            " 50% 25/50 [08:21<08:19, 19.96s/it]\u001b[A\n",
            " 52% 26/50 [08:45<08:26, 21.09s/it]\u001b[A\n",
            " 54% 27/50 [09:04<07:54, 20.64s/it]\u001b[A\n",
            " 56% 28/50 [09:24<07:27, 20.34s/it]\u001b[A\n",
            " 58% 29/50 [09:43<07:01, 20.09s/it]\u001b[A\n",
            " 60% 30/50 [10:03<06:39, 19.95s/it]\u001b[A\n",
            " 62% 31/50 [10:23<06:17, 19.86s/it]\u001b[A\n",
            " 64% 32/50 [10:44<06:07, 20.41s/it]\u001b[A\n",
            " 66% 33/50 [11:04<05:43, 20.18s/it]\u001b[A\n",
            " 68% 34/50 [11:23<05:19, 19.99s/it]\u001b[A\n",
            " 70% 35/50 [11:43<04:57, 19.85s/it]\u001b[A\n",
            " 72% 36/50 [12:04<04:41, 20.10s/it]\u001b[A\n",
            " 74% 37/50 [12:24<04:23, 20.29s/it]\u001b[A\n",
            " 76% 38/50 [12:46<04:09, 20.77s/it]\u001b[A\n",
            " 78% 39/50 [13:06<03:44, 20.39s/it]\u001b[A\n",
            " 80% 40/50 [13:25<03:21, 20.14s/it]\u001b[A\n",
            " 82% 41/50 [13:45<02:59, 19.96s/it]\u001b[A\n",
            " 84% 42/50 [14:05<02:38, 19.86s/it]\u001b[A\n",
            " 86% 43/50 [14:24<02:18, 19.79s/it]\u001b[A\n",
            " 88% 44/50 [14:46<02:01, 20.33s/it]\u001b[A\n",
            " 90% 45/50 [15:05<01:40, 20.08s/it]\u001b[A\n",
            " 92% 46/50 [15:25<01:19, 19.92s/it]\u001b[A\n",
            " 94% 47/50 [15:44<00:59, 19.82s/it]\u001b[A\n",
            " 96% 48/50 [16:04<00:39, 19.76s/it]\u001b[A\n",
            " 98% 49/50 [16:24<00:19, 19.71s/it]\u001b[A\n",
            "100% 50/50 [16:47<00:00, 20.16s/it]\n",
            "Epoch: 0, step: 25, loss: 0.477958\n",
            "Epoch: 0, step: 50, loss: 0.410701\n",
            "Epoch: 0, step: 75, loss: 0.326711\n",
            "Epoch: 0, step: 100, loss: 0.310849\n",
            "Epoch: 0, step: 125, loss: 0.325953\n",
            "Epoch: 0, step: 150, loss: 0.210724\n",
            "Epoch: 0, step: 175, loss: 0.0270866\n",
            "Epoch: 0, step: 200, loss: -0.0907401\n",
            "Epoch: 0, step: 225, loss: -0.152054\n",
            "Epoch: 0, step: 250, loss: -0.180995\n",
            "Epoch: 0, step: 275, loss: -0.147274\n",
            "Epoch: 0, step: 300, loss: -0.128737\n",
            "Epoch: 0, step: 325, loss: -0.160142\n",
            "Epoch: 0, step: 350, loss: -0.217606\n",
            "Epoch: 0, step: 375, loss: -0.268402\n",
            "Epoch: 0, step: 400, loss: -0.20015\n",
            "Epoch: 0, step: 425, loss: -0.219084\n",
            "Epoch: 0, step: 450, loss: -0.22251\n",
            "Epoch: 0, step: 475, loss: -0.219444\n",
            "Epoch: 0, step: 500, loss: -0.249628\n",
            "Sampling...\n",
            " 64% 500/781 [9:13:16<4:57:10, 63.45s/it]\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 1/50 [00:19<15:56, 19.53s/it]\u001b[A\n",
            "  4% 2/50 [00:39<15:36, 19.51s/it]\u001b[A\n",
            "  6% 3/50 [00:58<15:18, 19.53s/it]\u001b[A\n",
            "  8% 4/50 [01:18<14:59, 19.55s/it]\u001b[A\n",
            " 10% 5/50 [01:37<14:40, 19.57s/it]\u001b[A\n",
            " 12% 6/50 [01:57<14:20, 19.56s/it]\u001b[A\n",
            " 14% 7/50 [02:16<14:00, 19.55s/it]\u001b[A\n",
            " 16% 8/50 [02:36<13:40, 19.54s/it]\u001b[A\n",
            " 18% 9/50 [02:55<13:21, 19.56s/it]\u001b[A\n",
            " 20% 10/50 [03:15<13:02, 19.56s/it]\u001b[A\n",
            " 22% 11/50 [03:35<12:42, 19.56s/it]\u001b[A\n",
            " 24% 12/50 [03:54<12:23, 19.56s/it]\u001b[A\n",
            " 26% 13/50 [04:14<12:10, 19.73s/it]\u001b[A\n",
            " 28% 14/50 [04:34<11:47, 19.66s/it]\u001b[A\n",
            " 30% 15/50 [04:57<12:05, 20.73s/it]\u001b[A\n",
            " 32% 16/50 [05:17<11:33, 20.39s/it]\u001b[A\n",
            " 34% 17/50 [05:36<11:05, 20.18s/it]\u001b[A\n",
            " 36% 18/50 [05:56<10:40, 20.01s/it]\u001b[A\n",
            " 38% 19/50 [06:16<10:16, 19.90s/it]\u001b[A\n",
            " 40% 20/50 [06:35<09:54, 19.82s/it]\u001b[A\n",
            " 42% 21/50 [06:55<09:33, 19.78s/it]\u001b[A\n",
            " 44% 22/50 [07:14<09:12, 19.72s/it]\u001b[A\n",
            " 46% 23/50 [07:34<08:51, 19.69s/it]\u001b[A\n",
            " 48% 24/50 [07:54<08:31, 19.66s/it]\u001b[A\n",
            " 50% 25/50 [08:13<08:10, 19.63s/it]\u001b[A\n",
            " 52% 26/50 [08:33<07:50, 19.62s/it]\u001b[A\n",
            " 54% 27/50 [08:52<07:31, 19.61s/it]\u001b[A\n",
            " 56% 28/50 [09:12<07:11, 19.61s/it]\u001b[A\n",
            " 58% 29/50 [09:32<06:51, 19.60s/it]\u001b[A\n",
            " 60% 30/50 [09:51<06:32, 19.61s/it]\u001b[A\n",
            " 62% 31/50 [10:11<06:12, 19.59s/it]\u001b[A\n",
            " 64% 32/50 [10:30<05:52, 19.59s/it]\u001b[A\n",
            " 66% 33/50 [10:50<05:32, 19.58s/it]\u001b[A\n",
            " 68% 34/50 [11:09<05:12, 19.55s/it]\u001b[A\n",
            " 70% 35/50 [11:29<04:53, 19.56s/it]\u001b[A\n",
            " 72% 36/50 [11:49<04:33, 19.57s/it]\u001b[A\n",
            " 74% 37/50 [12:08<04:14, 19.55s/it]\u001b[A\n",
            " 76% 38/50 [12:28<03:54, 19.54s/it]\u001b[A\n",
            " 78% 39/50 [12:47<03:34, 19.54s/it]\u001b[A\n",
            " 80% 40/50 [13:07<03:15, 19.55s/it]\u001b[A\n",
            " 82% 41/50 [13:26<02:55, 19.55s/it]\u001b[A\n",
            " 84% 42/50 [13:46<02:36, 19.57s/it]\u001b[A\n",
            " 86% 43/50 [14:05<02:17, 19.58s/it]\u001b[A\n",
            " 88% 44/50 [14:25<01:57, 19.62s/it]\u001b[A\n",
            " 90% 45/50 [14:45<01:38, 19.61s/it]\u001b[A\n",
            " 92% 46/50 [15:04<01:18, 19.60s/it]\u001b[A\n",
            " 94% 47/50 [15:24<00:58, 19.60s/it]\u001b[A\n",
            " 96% 48/50 [15:44<00:39, 19.62s/it]\u001b[A\n",
            " 98% 49/50 [16:03<00:19, 19.59s/it]\u001b[A\n",
            "100% 50/50 [16:23<00:00, 19.66s/it]\n",
            "Epoch: 0, step: 525, loss: -0.232823\n",
            "Epoch: 0, step: 550, loss: -0.275343\n",
            "Epoch: 0, step: 575, loss: -0.273089\n",
            "Epoch: 0, step: 600, loss: -0.296635\n",
            " 80% 623/781 [11:32:15<2:29:10, 56.65s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate image"
      ],
      "metadata": {
        "id": "iGUrmB-pWb-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Installing dependencies"
      ],
      "metadata": {
        "id": "fgC8qKKEWlve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Jack000/glid-3-xl\n",
        "!cd glid-3-xl\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "bed5m44mWgNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Download models"
      ],
      "metadata": {
        "id": "D0e9a-_YXAUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# text encoder (required)\n",
        "!wget https://dall-3.com/models/glid-3-xl/bert.pt\n",
        "\n",
        "# ldm first stage (required)\n",
        "!wget https://dall-3.com/models/glid-3-xl/kl-f8.pt\n",
        "\n",
        "# original diffusion model from CompVis, we can also use our diffusion model trained\n",
        "wget https://dall-3.com/models/glid-3-xl/diffusion.pt"
      ],
      "metadata": {
        "id": "MREWBHq4WphD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Generating images"
      ],
      "metadata": {
        "id": "WbYkLfTWXMfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier free guidance + CLIP guidance (better adherence to prompt, much slower)\n",
        "!python sample.py --clip_guidance --model_path finetune.pt --batch_size 1 --num_batches 12 --text \"a cyberpunk girl with a scifi neuralink device on her head | trending on artstation\"\n",
        "# improve the quality of the image\n",
        "!python autoedit.py --edit image.png --model_path inpaint.pt --batch_size 6 --text \"a cyberpunk girl with a scifi neuralink device on her head | trending on artstation\""
      ],
      "metadata": {
        "id": "cEFnYqbOXOLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLIP interference with Stable diffusion pipeline on our trained model**"
      ],
      "metadata": {
        "id": "yBJ1pK8sD-dE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Define the pipeline"
      ],
      "metadata": {
        "id": "m3UMxPBcEMKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from diffusers import AutoencoderKL, DiffusionPipeline, LMSDiscreteScheduler, PNDMScheduler, UNet2DConditionModel\n",
        "from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion import StableDiffusionPipelineOutput\n",
        "from torchvision import transforms\n",
        "from transformers import CLIPFeatureExtractor, CLIPModel, CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cut_power=1.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cut_size = cut_size\n",
        "        self.cut_power = cut_power\n",
        "\n",
        "    def forward(self, pixel_values, num_cutouts):\n",
        "        sideY, sideX = pixel_values.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(num_cutouts):\n",
        "            size = int(torch.rand([]) ** self.cut_power * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = pixel_values[:, :, offsety : offsety + size, offsetx : offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "def set_requires_grad(model, value):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = value\n",
        "\n",
        "class CLIPGuidedStableDiffusion(DiffusionPipeline):\n",
        "    \"\"\"CLIP guided stable diffusion based on the amazing repo by @crowsonkb and @Jack000\n",
        "    - https://github.com/Jack000/glid-3-xl\n",
        "    - https://github.dev/crowsonkb/k-diffusion\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vae: AutoencoderKL,\n",
        "        text_encoder: CLIPTextModel,\n",
        "        clip_model: CLIPModel,\n",
        "        tokenizer: CLIPTokenizer,\n",
        "        unet: UNet2DConditionModel,\n",
        "        scheduler: Union[PNDMScheduler, LMSDiscreteScheduler],\n",
        "        feature_extractor: CLIPFeatureExtractor,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.register_modules(\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            clip_model=clip_model,\n",
        "            tokenizer=tokenizer,\n",
        "            unet=unet,\n",
        "            scheduler=scheduler,\n",
        "            feature_extractor=feature_extractor,\n",
        "        )\n",
        "\n",
        "        self.normalize = transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
        "        self.make_cutouts = MakeCutouts(feature_extractor.size)\n",
        "\n",
        "        set_requires_grad(self.text_encoder, False)\n",
        "        set_requires_grad(self.clip_model, False)\n",
        "\n",
        "    def enable_attention_slicing(self, slice_size: Optional[Union[str, int]] = \"auto\"):\n",
        "        if slice_size == \"auto\":\n",
        "            # half the attention head size is usually a good trade-off between\n",
        "            # speed and memory\n",
        "            slice_size = self.unet.config.attention_head_dim // 2\n",
        "        self.unet.set_attention_slice(slice_size)\n",
        "\n",
        "    def disable_attention_slicing(self):\n",
        "        self.enable_attention_slicing(None)\n",
        "\n",
        "    def freeze_vae(self):\n",
        "        set_requires_grad(self.vae, False)\n",
        "\n",
        "    def unfreeze_vae(self):\n",
        "        set_requires_grad(self.vae, True)\n",
        "\n",
        "    def freeze_unet(self):\n",
        "        set_requires_grad(self.unet, False)\n",
        "\n",
        "    def unfreeze_unet(self):\n",
        "        set_requires_grad(self.unet, True)\n",
        "\n",
        "    @torch.enable_grad()\n",
        "    def cond_fn(\n",
        "        self,\n",
        "        latents,\n",
        "        timestep,\n",
        "        index,\n",
        "        text_embeddings,\n",
        "        noise_pred_original,\n",
        "        text_embeddings_clip,\n",
        "        clip_guidance_scale,\n",
        "        num_cutouts,\n",
        "        use_cutouts=True,\n",
        "    ):\n",
        "        latents = latents.detach().requires_grad_()\n",
        "\n",
        "        if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            sigma = self.scheduler.sigmas[index]\n",
        "            # the model input needs to be scaled to match the continuous ODE formulation in K-LMS\n",
        "            latent_model_input = latents / ((sigma**2 + 1) ** 0.5)\n",
        "        else:\n",
        "            latent_model_input = latents\n",
        "\n",
        "        # predict the noise residual\n",
        "        noise_pred = self.unet(latent_model_input, timestep, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "        if isinstance(self.scheduler, PNDMScheduler):\n",
        "            alpha_prod_t = self.scheduler.alphas_cumprod[timestep]\n",
        "            beta_prod_t = 1 - alpha_prod_t\n",
        "            # compute predicted original sample from predicted noise also called\n",
        "            # \"predicted x_0\" of formula (12) from https://arxiv.org/pdf/2010.02502.pdf\n",
        "            pred_original_sample = (latents - beta_prod_t ** (0.5) * noise_pred) / alpha_prod_t ** (0.5)\n",
        "\n",
        "            fac = torch.sqrt(beta_prod_t)\n",
        "            sample = pred_original_sample * (fac) + latents * (1 - fac)\n",
        "        elif isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            sigma = self.scheduler.sigmas[index]\n",
        "            sample = latents - sigma * noise_pred\n",
        "        else:\n",
        "            raise ValueError(f\"scheduler type {type(self.scheduler)} not supported\")\n",
        "\n",
        "        sample = 1 / 0.18215 * sample\n",
        "        image = self.vae.decode(sample).sample\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "\n",
        "        if use_cutouts:\n",
        "            image = self.make_cutouts(image, num_cutouts)\n",
        "        else:\n",
        "            image = transforms.Resize(self.feature_extractor.size)(image)\n",
        "        image = self.normalize(image)\n",
        "\n",
        "        image_embeddings_clip = self.clip_model.get_image_features(image).float()\n",
        "        image_embeddings_clip = image_embeddings_clip / image_embeddings_clip.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "        if use_cutouts:\n",
        "            dists = spherical_dist_loss(image_embeddings_clip, text_embeddings_clip)\n",
        "            dists = dists.view([num_cutouts, sample.shape[0], -1])\n",
        "            loss = dists.sum(2).mean(0).sum() * clip_guidance_scale\n",
        "        else:\n",
        "            loss = spherical_dist_loss(image_embeddings_clip, text_embeddings_clip).mean() * clip_guidance_scale\n",
        "\n",
        "        grads = -torch.autograd.grad(loss, latents)[0]\n",
        "\n",
        "        if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            latents = latents.detach() + grads * (sigma**2)\n",
        "            noise_pred = noise_pred_original\n",
        "        else:\n",
        "            noise_pred = noise_pred_original - torch.sqrt(beta_prod_t) * grads\n",
        "        return noise_pred, latents\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]],\n",
        "        height: Optional[int] = 512,\n",
        "        width: Optional[int] = 512,\n",
        "        num_inference_steps: Optional[int] = 50,\n",
        "        guidance_scale: Optional[float] = 7.5,\n",
        "        clip_guidance_scale: Optional[float] = 100,\n",
        "        clip_prompt: Optional[Union[str, List[str]]] = None,\n",
        "        num_cutouts: Optional[int] = 4,\n",
        "        use_cutouts: Optional[bool] = True,\n",
        "        generator: Optional[torch.Generator] = None,\n",
        "        latents: Optional[torch.FloatTensor] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "        return_dict: bool = True,\n",
        "    ):\n",
        "        if isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "        if height % 8 != 0 or width % 8 != 0:\n",
        "            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
        "\n",
        "        # get prompt text embeddings\n",
        "        text_input = self.tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "        if clip_guidance_scale > 0:\n",
        "            if clip_prompt is not None:\n",
        "                clip_text_input = self.tokenizer(\n",
        "                    clip_prompt,\n",
        "                    padding=\"max_length\",\n",
        "                    max_length=self.tokenizer.model_max_length,\n",
        "                    truncation=True,\n",
        "                    return_tensors=\"pt\",\n",
        "                ).input_ids.to(self.device)\n",
        "            else:\n",
        "                clip_text_input = text_input.input_ids.to(self.device)\n",
        "            text_embeddings_clip = self.clip_model.get_text_features(clip_text_input)\n",
        "            text_embeddings_clip = text_embeddings_clip / text_embeddings_clip.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "        if do_classifier_free_guidance:\n",
        "            max_length = text_input.input_ids.shape[-1]\n",
        "            uncond_input = self.tokenizer(\n",
        "                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "            )\n",
        "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "            # For classifier free guidance, we need to do two forward passes.\n",
        "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "            # to avoid doing two forward passes\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "        # get the initial random noise unless the user supplied it\n",
        "\n",
        "        # Unlike in other pipelines, latents need to be generated in the target device\n",
        "        # for 1-to-1 results reproducibility with the CompVis implementation.\n",
        "        # However this currently doesn't work in `mps`.\n",
        "        latents_device = \"cpu\" if self.device.type == \"mps\" else self.device\n",
        "        latents_shape = (batch_size, self.unet.in_channels, height // 8, width // 8)\n",
        "        if latents is None:\n",
        "            latents = torch.randn(\n",
        "                latents_shape,\n",
        "                generator=generator,\n",
        "                device=latents_device,\n",
        "            )\n",
        "        else:\n",
        "            if latents.shape != latents_shape:\n",
        "                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {latents_shape}\")\n",
        "        latents = latents.to(self.device)\n",
        "\n",
        "        # set timesteps\n",
        "        accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "        extra_set_kwargs = {}\n",
        "        if accepts_offset:\n",
        "            extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "        self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "        # if we use LMSDiscreteScheduler, let's make sure latents are multiplied by sigmas\n",
        "        if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "            latents = latents * self.scheduler.sigmas[0]\n",
        "\n",
        "        for i, t in enumerate(self.progress_bar(self.scheduler.timesteps)):\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "            if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "                sigma = self.scheduler.sigmas[i]\n",
        "                # the model input needs to be scaled to match the continuous ODE formulation in K-LMS\n",
        "                latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "            # predict the noise residual\n",
        "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "            # perform classifier free guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # perform clip guidance\n",
        "            if clip_guidance_scale > 0:\n",
        "                text_embeddings_for_guidance = (\n",
        "                    text_embeddings.chunk(2)[1] if do_classifier_free_guidance else text_embeddings\n",
        "                )\n",
        "                noise_pred, latents = self.cond_fn(\n",
        "                    latents,\n",
        "                    t,\n",
        "                    i,\n",
        "                    text_embeddings_for_guidance,\n",
        "                    noise_pred,\n",
        "                    text_embeddings_clip,\n",
        "                    clip_guidance_scale,\n",
        "                    num_cutouts,\n",
        "                    use_cutouts,\n",
        "                )\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            if isinstance(self.scheduler, LMSDiscreteScheduler):\n",
        "                latents = self.scheduler.step(noise_pred, i, latents).prev_sample\n",
        "            else:\n",
        "                latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
        "\n",
        "        # scale and decode the image latents with vae\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        image = self.vae.decode(latents).sample\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        if not return_dict:\n",
        "            return (image, None)\n",
        "\n",
        "        return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=None)"
      ],
      "metadata": {
        "id": "quitvMRWEFSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Load the pipeline"
      ],
      "metadata": {
        "id": "D6Tn-H8pEcpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import LMSDiscreteScheduler, StableDiffusionPipeline\n",
        "from PIL import Image\n",
        "from transformers import CLIPFeatureExtractor, CLIPModel\n",
        "\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\" #@param {type: \"string\"}\n",
        "clip_model_id = \"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\" #@param [\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\", \"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\", \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\", \"laion/CLIP-ViT-g-14-laion2B-s12B-b42K\", \"openai/clip-vit-base-patch32\", \"openai/clip-vit-base-patch16\", \"openai/clip-vit-large-patch14\"] {allow-input: true}\n",
        "scheduler = \"plms\" #@param ['plms', 'lms']\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "    \n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid\n",
        "\n",
        "def create_clip_guided_pipeline(\n",
        "    model_id=\"CompVis/stable-diffusion-v1-4\", clip_model_id=\"laion/CLIP-ViT-B-32-laion2B-s34B-b79K\", scheduler=\"plms\"\n",
        "):\n",
        "    pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.float16,\n",
        "        revision=\"fp16\",\n",
        "        use_auth_token=True,\n",
        "    )\n",
        "\n",
        "    if scheduler == \"lms\":\n",
        "        scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\")\n",
        "    else:\n",
        "        scheduler = pipeline.scheduler\n",
        "\n",
        "    clip_model = CLIPModel.from_pretrained(clip_model_id, torch_dtype=torch.float16)\n",
        "    feature_extractor = CLIPFeatureExtractor.from_pretrained(clip_model_id, torch_dtype=torch.float16)\n",
        "\n",
        "    guided_pipeline = CLIPGuidedStableDiffusion(\n",
        "        unet=pipeline.unet,\n",
        "        vae=pipeline.vae,\n",
        "        tokenizer=pipeline.tokenizer,\n",
        "        text_encoder=pipeline.text_encoder,\n",
        "        scheduler=scheduler,\n",
        "        clip_model=clip_model,\n",
        "        feature_extractor=feature_extractor,\n",
        "    )\n",
        "\n",
        "    return guided_pipeline\n",
        "\n",
        "guided_pipeline = create_clip_guided_pipeline(model_id, clip_model_id)\n",
        "guided_pipeline = guided_pipeline.to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "d7516LHyFEOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Generate image on google colab"
      ],
      "metadata": {
        "id": "OGPOzAcLGALf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"fantasy book cover, full moon, fantasy forest landscape, golden vector elements, fantasy magic, dark light night, intricate, elegant, sharp focus, illustration, highly detailed, digital painting, concept art, matte, art by WLOP and Artgerm and Albert Bierstadt, masterpiece\" #@param {type: \"string\"}\n",
        "#@markdown `clip_prompt` is optional, if you leave it blank the same prompt is sent to Stable Diffusion and CLIP\n",
        "clip_prompt = \"\" #@param {type: \"string\"}\n",
        "num_samples = 1 #@param {type: \"number\"}\n",
        "num_inference_steps = 50 #@param {type: \"number\"}\n",
        "guidance_scale = 7.5 #@param {type: \"number\"}\n",
        "clip_guidance_scale = 100 #@param {type: \"number\"}\n",
        "num_cutouts = 4 #@param {type: \"number\"}\n",
        "use_cutouts = \"False\" #@param [\"False\", \"True\"]\n",
        "unfreeze_unet = \"True\" #@param [\"False\", \"True\"]\n",
        "unfreeze_vae = \"True\" #@param [\"False\", \"True\"]\n",
        "seed = 3788086447 #@param {type: \"number\"}\n",
        "\n",
        "if unfreeze_unet == \"True\":\n",
        "  guided_pipeline.unfreeze_unet()\n",
        "else:\n",
        "  guided_pipeline.freeze_unet()\n",
        "\n",
        "if unfreeze_vae == \"True\":\n",
        "  guided_pipeline.unfreeze_vae()\n",
        "else:\n",
        "  guided_pipeline.freeze_vae()\n",
        "\n",
        "generator = torch.Generator(device=\"cuda\").manual_seed(seed)\n",
        "\n",
        "images = []\n",
        "for i in range(num_samples):\n",
        "    with torch.autocast(\"cuda\"):\n",
        "        image = guided_pipeline(\n",
        "            prompt,\n",
        "            clip_prompt=clip_prompt if clip_prompt.strip() != \"\" else None,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale, \n",
        "            clip_guidance_scale=clip_guidance_scale,\n",
        "            num_cutouts=num_cutouts,\n",
        "            use_cutouts=use_cutouts == \"True\",\n",
        "            generator=generator,\n",
        "        ).images[0]\n",
        "    images.append(image)\n",
        "\n",
        "image_grid(images, 1, num_samples)"
      ],
      "metadata": {
        "id": "r1IR_WgSGE-F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}